{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d2a3c8-bfdf-4e9b-a236-7a7ad95dbfc6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d8e878-3527-4cd8-9a96-2958763eb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from oggm import utils, workflow, cfg\n",
    "import numpy as np\n",
    "from agile1d.sandbox.calculate_statistics import add_1d_stats\n",
    "from agile1d.sandbox.create_glaciers_with_measurements import create_idealized_experiments\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447630a-dd0a-4535-9c60-0aba2bf42928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gdirs to get first guess later\n",
    "cfg.initialize()\n",
    "cfg.PARAMS['border'] = 160\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "\n",
    "WORKING_DIR = os.environ[\"OGGM_WORKDIR\"]\n",
    "utils.mkdir(WORKING_DIR)\n",
    "cfg.PATHS['working_dir'] = WORKING_DIR\n",
    "\n",
    "from_prepro_level = 2\n",
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/' \\\n",
    "           'L1-L2_files/elev_bands/'\n",
    "gcm='BCC-CSM2-MR'\n",
    "ssp='ssp370'\n",
    "glaciers = ['Aletsch', 'Baltoro', 'Artesonraju', 'Peyto']\n",
    "glacier_states = ['equilibrium', 'retreating', 'advancing']\n",
    "\n",
    "gdirs = create_idealized_experiments(glaciers,\n",
    "                                 glacier_states,\n",
    "                                 prepro_border=cfg.PARAMS['border'],\n",
    "                                 from_prepro_level=from_prepro_level,\n",
    "                                 base_url=base_url,\n",
    "                                 gcm=gcm, ssp=ssp)\n",
    "\n",
    "def change_dict_key(d, old_key, new_key, default_value=None):\n",
    "    if old_key[-1] == '_':\n",
    "        pass\n",
    "    else:\n",
    "        d[new_key] = d.pop(old_key, default_value)\n",
    "\n",
    "def get_files_containing_phrase(all_files, phrases):\n",
    "    return_files = copy.deepcopy(all_files)\n",
    "    for phrase in phrases:\n",
    "        return_files = [file for file in return_files if phrase in file]\n",
    "    return return_files\n",
    "\n",
    "def all_score_line_plots(glaciers, glacier_states, features, measure, input_folder,\n",
    "                         extra_phrases=[],\n",
    "                             experiment_description='',\n",
    "                             compare_to_fg=True,  # False, True, 'dynamic', 'static'\n",
    "                             length_top_list=10, length_worst_list=10,\n",
    "                             filename='', output_folder='', save_fig=True,\n",
    "                             add_pseudo_3d=False, add_feature_importance_plot=False,\n",
    "                             add_l_curve_matrix=False, add_score_line=True,\n",
    "                             add_score_board=False):\n",
    "    # get all files of input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    \n",
    "    # create a template for the feature scores\n",
    "    feature_scores_tmpl = {}\n",
    "    for main_feature in features:\n",
    "        for abbreviation in features[main_feature]:\n",
    "            feature_scores_tmpl[abbreviation] = 0\n",
    "\n",
    "    # function to go recusivly through features and sum up score, also maintain top list,\n",
    "    # get Jobs and Jreg for L-Curve\n",
    "    def open_file_and_get_score(phrases, features, feature_scores, feature_super_score, feature_bad_count,\n",
    "                                feature_time_of_occurence,\n",
    "                                list_of_all_scores, gdir, super_score, l_curve_data, glacier_state):\n",
    "        keys = list(features.keys())\n",
    "        \n",
    "        current_feature = keys[0]\n",
    "        for realisation in features[current_feature]:\n",
    "            current_phrases = copy.deepcopy(phrases)\n",
    "            current_phrases.append(realisation)\n",
    "\n",
    "            # if their are more than one feature left we call the function again\n",
    "            if len(keys) > 1:\n",
    "                next_features = copy.deepcopy(features)\n",
    "                next_features.pop(current_feature)\n",
    "                \n",
    "                open_file_and_get_score(\n",
    "                    current_phrases, next_features, feature_scores, feature_super_score, feature_bad_count,\n",
    "                    feature_time_of_occurence,\n",
    "                    list_of_all_scores, gdir, super_score, l_curve_data, glacier_state)\n",
    "\n",
    "            # ok we are left with one feature, so open file and get score\n",
    "            else:\n",
    "                file_name = get_files_containing_phrase(all_files, current_phrases)\n",
    "                if len(file_name) > 1:\n",
    "                    raise NotImplementedError(f'more than one filename found with '\n",
    "                                              f'{current_phrases}: {file_name}! '\n",
    "                                              'Need to be more specific!')\n",
    "                elif len(file_name) == 0:\n",
    "                    raise NotImplementedError(f'no filename found with {current_phrases}!')\n",
    "                with open(os.path.join(input_folder, file_name[0]), 'rb') as handle:\n",
    "                    open_file = pickle.load(handle)\n",
    "\n",
    "                try:\n",
    "                    if measure == 'mad_bed_h':\n",
    "                        current_score = open_file.attrs['controls_stats']['area_bed_h']['mean_ad']\n",
    "                    elif measure == 'mad_today_volume':\n",
    "                        # need to recompute because of a bug\n",
    "                        fl_mdl_end = open_file.flowlines[-1].item()\n",
    "                        fl_true_end = gdir.read_pickle('model_flowlines',\n",
    "                                                       filesuffix=f'_agile_true_end_{glacier_state}')[0]\n",
    "                        current_score = add_1d_stats(fl_mdl_end.section * fl_mdl_end.dx_meter,\n",
    "                                                     fl_true_end.section * fl_true_end.dx_meter)['mean_ad']\n",
    "                    elif measure == 'mad_past_volume':\n",
    "                        current_score = open_file.attrs['past_state_stats']['volume_m3']['mean_ad']\n",
    "                    elif measure == 'total_today_volume':\n",
    "                        # need to recompute because of a bug\n",
    "                        fl_mdl_end = open_file.flowlines[-1].item()\n",
    "                        fl_true_end = gdir.read_pickle('model_flowlines',\n",
    "                                                       filesuffix=f'_agile_true_end_{glacier_state}')[0]\n",
    "                        current_score = np.abs(np.sum(\n",
    "                            add_1d_stats(fl_mdl_end.section * fl_mdl_end.dx_meter,\n",
    "                                         fl_true_end.section * fl_true_end.dx_meter)['diff']))\n",
    "                    else:\n",
    "                        raise NotImplementedError(f'{measure}')\n",
    "                except:\n",
    "                    current_score = np.inf\n",
    "                \n",
    "                # add individual score\n",
    "                list_of_all_scores.append((file_name[0], current_score))\n",
    "\n",
    "                if add_l_curve_matrix:\n",
    "                    # extract L-Curve data\n",
    "                    l_curve_data[file_name[0]] = {\n",
    "                        'lambda': open_file.attrs['cost_lambda'],\n",
    "                        'Jobs': open_file.c_terms_description[-1].item()['J_obs'],\n",
    "                        'Jreg': open_file.c_terms_description[-1].item()['J_reg'],\n",
    "                    }\n",
    "\n",
    "    # loop thourgh all glaciers and calculate scores, and \n",
    "    for glacier in glaciers:\n",
    "        for glacier_state in glacier_states:\n",
    "            feature_scores = copy.deepcopy(feature_scores_tmpl)\n",
    "            \n",
    "            # create a dict for super scores, 0 = not defined, 1 = not good, 2 = good\n",
    "            feature_super_score = copy.deepcopy(feature_scores_tmpl)\n",
    "    \n",
    "            # count the number of occurence below super score\n",
    "            feature_bad_count = copy.deepcopy(feature_scores_tmpl)\n",
    "            feature_time_of_occurence = copy.deepcopy(feature_scores_tmpl)\n",
    "    \n",
    "            list_of_all_scores = []\n",
    "            l_curve_data = {}\n",
    "    \n",
    "            rgi_id = {'Baltoro': 'RGI60-14.06794', 'Aletsch': 'RGI60-11.01450',\n",
    "                      'Artesonraju': 'RGI60-16.02444', 'Peyto': 'RGI60-02.05098',\n",
    "                     }[glacier]\n",
    "            for gdir_tmp in gdirs:\n",
    "                if gdir_tmp.rgi_id == rgi_id:\n",
    "                    gdir = gdir_tmp\n",
    "    \n",
    "            if compare_to_fg:\n",
    "                # add first guess run scores\n",
    "                with open(os.path.join(gdir.dir, f'default_oggm_statistics_{glacier_state}.pkl'), 'rb') as handle:\n",
    "                    oggm_default_stats = pickle.load(handle)\n",
    "    \n",
    "                super_score = None\n",
    "                if measure == 'mad_bed_h':\n",
    "                    stats = 'controls_stats'\n",
    "                    for run, stats_suffix in zip(['oggm_dynamic_spinup', 'oggm_fixed_geometry'],\n",
    "                                                 ['_dynamic', '_static']):\n",
    "                        score_tmp = oggm_default_stats[stats + stats_suffix]['bed_h']['mean_ad']\n",
    "                        list_of_all_scores.append((run, score_tmp))\n",
    "    \n",
    "                        if stats_suffix == '_static':\n",
    "                            super_score = score_tmp\n",
    "    \n",
    "                elif measure == 'mad_today_volume':\n",
    "                    stats = 'today_state_stats'\n",
    "                    for run, stats_suffix in zip(['oggm_dynamic_spinup', 'oggm_fixed_geometry'],\n",
    "                                                 ['_dynamic', '_static']):\n",
    "                        score_tmp = oggm_default_stats[stats + stats_suffix]['volume_m3']['mean_ad']\n",
    "                        list_of_all_scores.append((run, score_tmp))\n",
    "    \n",
    "                        if stats_suffix == '_static':\n",
    "                            super_score = score_tmp\n",
    "    \n",
    "                elif measure == 'mad_past_volume':\n",
    "                    stats = 'past_state_stats'\n",
    "                    for run, stats_suffix in zip(['oggm_dynamic_spinup', 'oggm_fixed_geometry'],\n",
    "                                                 ['_dynamic', '_static']):\n",
    "                        score_tmp = oggm_default_stats[stats + stats_suffix]['volume_m3']['mean_ad']\n",
    "                        list_of_all_scores.append((run, score_tmp))\n",
    "    \n",
    "                        if stats_suffix == '_static':\n",
    "                            super_score = score_tmp\n",
    "    \n",
    "                elif measure == 'total_today_volume':\n",
    "                    stats = 'today_state_stats'\n",
    "                    for run, stats_suffix in zip(['oggm_dynamic_spinup', 'oggm_fixed_geometry'],\n",
    "                                                 ['_dynamic', '_static']):\n",
    "                        score_tmp = np.abs(np.sum(oggm_default_stats[stats + stats_suffix]['volume_m3']['diff']))\n",
    "                        list_of_all_scores.append((run, score_tmp))\n",
    "    \n",
    "                        if stats_suffix == '_static':\n",
    "                            super_score = score_tmp\n",
    "    \n",
    "                else:\n",
    "                    raise NotImplementedError(f'{measure}')\n",
    "    \n",
    "            # actually getting the scores and other stuff\n",
    "            phrases = [glacier, glacier_state, '.pkl']\n",
    "            for ex_phrase in extra_phrases:\n",
    "                phrases.append(ex_phrase)\n",
    "            open_file_and_get_score(phrases, features,\n",
    "                                    feature_scores, feature_super_score, feature_bad_count,\n",
    "                                    feature_time_of_occurence,\n",
    "                                    list_of_all_scores, gdir, super_score, l_curve_data, glacier_state)\n",
    "    \n",
    "            # sort all runs and assign ranking number\n",
    "            all_scores_sorted = [\n",
    "                (f'{i + 1}.', score[0], score[1])\n",
    "                for i, score in\n",
    "                enumerate(sorted(list_of_all_scores, key = lambda x: x[1], reverse=False))]\n",
    "    \n",
    "            # sort bad counts\n",
    "            feature_bad_count_list = []\n",
    "            for feature_tmp in feature_bad_count:\n",
    "                if feature_bad_count[feature_tmp] > 0:\n",
    "                    feature_bad_count_list.append((feature_tmp,\n",
    "                                                   feature_bad_count[feature_tmp],\n",
    "                                                   feature_time_of_occurence[feature_tmp]))\n",
    "            feature_bad_count_sorted = sorted(feature_bad_count_list, key = lambda x: x[1], reverse=True)\n",
    "            feature_bad_count_sorted = sorted(feature_bad_count_sorted, key = lambda x: x[0][0], reverse=True)\n",
    "    \n",
    "            if compare_to_fg:\n",
    "                # get rank number of first guess\n",
    "                oggm_dynamic_rank = None\n",
    "                oggm_static_rank = None\n",
    "                for score_tmp in all_scores_sorted:\n",
    "                    if score_tmp[1] == 'oggm_dynamic_spinup':\n",
    "                        oggm_dynamic_rank = int(float(score_tmp[0]))\n",
    "                    if score_tmp[1] == 'oggm_fixed_geometry':\n",
    "                        oggm_static_rank = int(float(score_tmp[0]))\n",
    "                    if oggm_dynamic_rank is not None:\n",
    "                        if oggm_static_rank is not None:\n",
    "                            break\n",
    "        \n",
    "                # get features which are always worse than first guess\n",
    "                feature_worse_fg = copy.deepcopy(feature_scores_tmpl)\n",
    "                if isinstance(compare_to_fg, str):\n",
    "                    if compare_to_fg == 'dynamic':\n",
    "                        max_score_to_compare = oggm_dynamic_rank\n",
    "                    elif compare_to_fg == 'static':\n",
    "                        max_score_to_compare = oggm_static_rank\n",
    "                    else:\n",
    "                        raise NotImplementedError(f'{compare_to_fg}')\n",
    "                else:\n",
    "                    max_score_to_compare = max(oggm_dynamic_rank, oggm_static_rank)\n",
    "                for score_tmp in all_scores_sorted:\n",
    "                    if int(float(score_tmp[0])) > max_score_to_compare:\n",
    "                        break\n",
    "                    for feature in feature_worse_fg:\n",
    "                        if feature in score_tmp[1]:\n",
    "                            feature_worse_fg[feature] = 1\n",
    "    \n",
    "            if add_l_curve_matrix:\n",
    "                all_obs = list(features['observations'].keys())\n",
    "                if 'regularisation_terms' in features.keys():\n",
    "                    all_reg = list(features['regularisation_terms'].keys())\n",
    "                else:\n",
    "                    all_reg = None\n",
    "                all_lam = list(features['cost_lambda'].keys())\n",
    "                \n",
    "                l_curve_phrases = [glacier, '.pkl']\n",
    "                add_single_value_features(l_curve_phrases, features)\n",
    "    \n",
    "                title = f'{glacier}, {experiment_description}, {measure}'\n",
    "                plot_l_curve_matrix(l_curve_phrases, l_curve_data, all_obs, all_reg, all_lam, title)\n",
    "    \n",
    "            if add_pseudo_3d:\n",
    "                if 'regularisation_terms' in features.keys():\n",
    "                    x = list(features['cost_lambda'].keys())\n",
    "                    z = list(features['observations'].keys())\n",
    "                    y = list(features['regularisation_terms'].keys())\n",
    "                else:\n",
    "                    x = list(features['cost_lambda'].keys())\n",
    "                    y = list(features['observations'].keys())\n",
    "                    z = None\n",
    "                ref = super_score\n",
    "                data = list_of_all_scores\n",
    "                sorted_data = all_scores_sorted\n",
    "                measure_title = {\n",
    "                    'mad_bed_h': 'MAD bed height',\n",
    "                    'mad_today_volume': 'MAD 2020 dist. volume',\n",
    "                }[measure]\n",
    "                title = f'{glacier}, {experiment_description}, {measure_title}'\n",
    "                plot_pseudo_3D(x, y, z, ref, data, sorted_data, title,\n",
    "                               add_text=True)\n",
    "\n",
    "            if add_score_line:\n",
    "                fig, ax = plt.subplots(figsize=(5, 2))\n",
    "\n",
    "                plot_score_line(ax, all_scores_sorted)\n",
    "\n",
    "                measure_title = {\n",
    "                    'mad_bed_h': 'MAD bed height in m',\n",
    "                    'mad_today_volume': 'MAD 2020 dist. volume in mÂ³',\n",
    "                }[measure]\n",
    "                ax.set_title(f'{glacier} {glacier_state}, {measure_title}')\n",
    "\n",
    "                if save_fig:\n",
    "                    fig.savefig(os.path.join(output_folder,\n",
    "                                             f'{glacier}_{glacier_state}_{extra_phrases[0]}_{measure}.png'), \n",
    "                                facecolor='white', bbox_inches='tight')\n",
    "                plt.show()\n",
    "    \n",
    "            if add_feature_importance_plot:\n",
    "                # create plot with feature box plots and top_score list\n",
    "                # depending on number of features create figure layout\n",
    "                fig_rows = int((len(features) - 1) / 3) + 1\n",
    "                \n",
    "                fig, axs = plt.subplots(fig_rows, 3, layout='constrained', figsize=(20, fig_rows * 2))\n",
    "                \n",
    "                fig.suptitle(f'{glacier} ({measure})')\n",
    "                for i, feature in enumerate(features.keys()):\n",
    "                    # get the individual score values\n",
    "                    score_labels = [abbre for abbre in features[feature]]\n",
    "                    score_values = [feature_scores[abbre] for abbre in features[feature]]\n",
    "                    if compare_to_fg:\n",
    "                        bar_colors = [{0: 'red', 1: 'blue'}[feature_worse_fg[abbre]]\n",
    "                                      for abbre in features[feature]]\n",
    "                    else:\n",
    "                        bar_colors = ['blue' for abbre in features[feature]]\n",
    "                    \n",
    "                    row = int(i / 3)\n",
    "                    col = i % 3\n",
    "                    if fig_rows > 1:\n",
    "                        ax = axs[row][col]\n",
    "                    else:\n",
    "                        ax = axs[col]\n",
    "                    \n",
    "                    ax.bar(score_labels, score_values, color=bar_colors)\n",
    "        \n",
    "                    ax.set_title(feature)\n",
    "                    ax.tick_params(axis='x', labelrotation=90)\n",
    "                    ax_range = max(score_values) - min(score_values)\n",
    "                    if ax_range == 0:\n",
    "                        ax_range = 1\n",
    "                    ax.set_ylim((min(score_values) - 0.05 * ax_range,\n",
    "                                 max(score_values) + 0.05 * ax_range))\n",
    "    \n",
    "            else:\n",
    "                if add_score_board:\n",
    "                    fig, axs = plt.subplots(1, 1, layout='constrained', figsize=(20, 1))\n",
    "\n",
    "            if add_score_board:\n",
    "            # add top score board at end of plot\n",
    "                top_scores = all_scores_sorted[:length_top_list]\n",
    "                final_str = 'Top Score List\\n'\n",
    "                for score in top_scores:\n",
    "                    final_str += str(score)\n",
    "                    final_str += '\\n'\n",
    "        \n",
    "                if compare_to_fg:\n",
    "                    final_str += f'\\n{all_scores_sorted[oggm_dynamic_rank-1]}\\n'\n",
    "                    final_str += f'{all_scores_sorted[oggm_static_rank-1]}\\n'\n",
    "        \n",
    "                worst_scores = all_scores_sorted[-length_worst_list:]\n",
    "                final_str += '\\nWorst Score List\\n'\n",
    "                for score in worst_scores:\n",
    "                    final_str += str(score)\n",
    "                    final_str += '\\n'\n",
    "        \n",
    "                # add super settings\n",
    "                final_str += f'\\nSuper Settings (always better than {print_super_settings} * fg = {super_score})\\n'\n",
    "                for feature_tmp in feature_super_score:\n",
    "                    if feature_super_score[feature_tmp] == 2:\n",
    "                        final_str += f'{feature_tmp}, '\n",
    "        \n",
    "                # print bad count\n",
    "                final_str += '\\n\\nCount of bad occurences (below super score)\\n'\n",
    "                for bad_item_tmp in feature_bad_count_sorted:\n",
    "                    final_str += f'\\n{bad_item_tmp}'\n",
    "                # add all option descritpions at the end as well\n",
    "                final_str += '\\n\\n'\n",
    "                final_str += 'Selected options\\n'\n",
    "                for feature in features:\n",
    "                    final_str += '\\n' + str(feature) + '\\n'\n",
    "                    for abbre in features[feature]:\n",
    "                        final_str += f'{abbre}: {features[feature][abbre]}\\n'\n",
    "        \n",
    "                plt.figtext(0.5, -0.02, final_str, horizontalalignment='center', verticalalignment='top', fontsize=15)\n",
    "                \n",
    "                if save_fig:\n",
    "                    utils.mkdir(output_folder)\n",
    "                    \n",
    "                    fig.savefig(os.path.join(output_folder, filename + f'_{glacier}.png'), \n",
    "                                facecolor='white', bbox_inches='tight')\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "    #return all_scores_sorted\n",
    "\n",
    "def plot_score_line(ax, sorted_list):\n",
    "    # get fg value to compere to\n",
    "    for entry in sorted_list:\n",
    "        if entry[1] == 'oggm_fixed_geometry':\n",
    "            fg_rank = entry[0]\n",
    "            fg_score = entry[2]\n",
    "            break\n",
    "    \n",
    "    # define plotting values with colors and labels\n",
    "    c1 = 'C0'\n",
    "    c2 = 'C1'\n",
    "    x = []\n",
    "    y = []\n",
    "    bar_colors = []\n",
    "    bar_labels = []\n",
    "    for entry in sorted_list:\n",
    "        x.append(entry[0])\n",
    "        y.append(entry[2] / fg_score)\n",
    "        if entry[2] >= fg_score:\n",
    "            bar_colors.append(c2)\n",
    "        else:\n",
    "            bar_colors.append(c1)\n",
    "        if entry[1] == 'oggm_dynamic_spinup':\n",
    "            bar_labels.append(f'OGGM\\nDYN\\n{entry[0]}')\n",
    "        elif entry[1] == 'oggm_fixed_geometry':\n",
    "            bar_labels.append(f'OGGM\\nSTAT\\n{entry[0]}')\n",
    "        else:\n",
    "            bar_labels.append('')\n",
    "    \n",
    "        if entry[2] == sorted_list[-1][2]:\n",
    "            if entry[1] not in ['oggm_dynamic_spinup', 'oggm_fixed_geometry']:\n",
    "                bar_labels[-1] = f'{bar_labels[-1]}\\n{entry[0]}'\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(10, 1))\n",
    "\n",
    "    ax.bar(x, y, color=bar_colors, width=1)\n",
    "    ax.axhline(1, zorder=0, color='gray', ls='--', lw=1)\n",
    "    #ax.text(float(x[0]), 1, 'OGGM DYN score', ha='right', va='center')\n",
    "    ax.set_xticks(x, bar_labels)\n",
    "    ax.set_xlim([float(x[0]), float(x[-1])])\n",
    "\n",
    "\n",
    "fg_options = [#'fg_glabtop',\n",
    "              'fg_oggm']\n",
    "\n",
    "# create feature dict\n",
    "start_ex = -4  # == 10^start_ex\n",
    "end_ex = 3\n",
    "nr_lam = 29\n",
    "all_lambdas = np.logspace(start_ex, end_ex, num=nr_lam)\n",
    "lam_dict = {'lam0': 0\n",
    "}\n",
    "for i, lam in enumerate(all_lambdas):\n",
    "    lam_dict[f'lam{i + 1}'] = lam\n",
    "\n",
    "features = \\\n",
    "    {\n",
    "        'cost_lambda': lam_dict,\n",
    "        'observations': {\n",
    "            'obs0': {'fl_surface_h:m': {}},\n",
    "            'obs1': {'dmdtda:kg m-2 yr-1': {}},\n",
    "            'obs2': {'volume:km3': {}},\n",
    "            'obs3': {'fl_surface_h:m': {},\n",
    "                     'dmdtda:kg m-2 yr-1': {}},\n",
    "            'obs4': {'fl_surface_h:m': {},\n",
    "                     'volume:km3': {}},\n",
    "            'obs5': {'dmdtda:kg m-2 yr-1': {},\n",
    "                     'volume:km3': {}},\n",
    "            'obs6': {'fl_surface_h:m': {},\n",
    "                     'dmdtda:kg m-2 yr-1': {},\n",
    "                     'volume:km3': {}},\n",
    "        },\n",
    "        'regularisation_terms': {\n",
    "            'reg0': {'smoothed_bed': 1},\n",
    "            'reg1': {'smoothed_flux': 0.1},\n",
    "            'reg2': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e-5},\n",
    "            'reg3': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e-4},\n",
    "            'reg4': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e-3},\n",
    "            'reg5': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e-2},\n",
    "            'reg6': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e-1},\n",
    "            'reg7': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1},\n",
    "            'reg8': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e1},\n",
    "            'reg9': {'smoothed_bed': 1,\n",
    "                      'smoothed_flux': 1e2},\n",
    "        },\n",
    "    }\n",
    "\n",
    "for key in copy.deepcopy(features['cost_lambda']):\n",
    "    change_dict_key(features['cost_lambda'],\n",
    "                    key,\n",
    "                    f'{key}_')\n",
    "\n",
    "input_folder = '/home/www/pschmitt/agile/final_runs_glacier_states/full_run/results/'\n",
    "output_folder = '/home/www/pschmitt/agile/final_runs_glacier_states/full_run/simple_score_plots_oggm_workshop24_updated_title/'\n",
    "\n",
    "for fg_method in fg_options:\n",
    "    for measure in ['mad_bed_h', 'mad_today_volume']:\n",
    "        all_scores_sorted = all_score_line_plots(glaciers, glacier_states, features, measure, input_folder,\n",
    "                            extra_phrases=[fg_method], output_folder=output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oggm_env]",
   "language": "python",
   "name": "conda-env-oggm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
